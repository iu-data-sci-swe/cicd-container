{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ef35f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('../data/fraud.csv.bz2')\n",
    "\n",
    "# Parse the time column as datetime\n",
    "df['time'] = pd.to_datetime(df['time'], format='%Y-%m-%d %H:%M:%S', errors='coerce')\n",
    "\n",
    "# Display basic information\n",
    "print(\"Dataset Shape:\")\n",
    "print(df.shape)\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"Dataset Info:\")\n",
    "print(df.info())\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b029ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup train / test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_cleaned = df.dropna().copy()\n",
    "\n",
    "X = df_cleaned.drop('fraud', axis=1)\n",
    "y = df_cleaned['fraud']\n",
    "\n",
    "# encode categorical variables\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "X = X.copy()\n",
    "y = y.copy()\n",
    "\n",
    "# add hour of day as a feature\n",
    "X['hour'] = X['time'].dt.hour\n",
    "\n",
    "# Fit OneHotEncoders on training data\n",
    "enc_product = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "enc_gender = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "enc_state = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "enc_hour = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "\n",
    "# Fit and transform - reshape needed because OneHotEncoder expects 2D input\n",
    "product_encoded = enc_product.fit_transform(X[['product_category']])\n",
    "gender_encoded = enc_gender.fit_transform(X[['gender']])\n",
    "state_encoded = enc_state.fit_transform(X[['address_state']])\n",
    "hour_encoded = enc_hour.fit_transform(X[['hour']])\n",
    "\n",
    "# Get feature names for the encoded columns\n",
    "product_cols = enc_product.get_feature_names_out(['product_category'])\n",
    "gender_cols = enc_gender.get_feature_names_out(['gender'])\n",
    "state_cols = enc_state.get_feature_names_out(['address_state'])\n",
    "hour_cols = enc_hour.get_feature_names_out(['hour'])\n",
    "\n",
    "# Create DataFrames from encoded arrays\n",
    "product_df = pd.DataFrame(product_encoded, columns=product_cols, index=X.index)\n",
    "gender_df = pd.DataFrame(gender_encoded, columns=gender_cols, index=X.index)\n",
    "state_df = pd.DataFrame(state_encoded, columns=state_cols, index=X.index)\n",
    "hour_df = pd.DataFrame(hour_encoded, columns=hour_cols, index=X.index)\n",
    "\n",
    "# Drop original columns and concatenate encoded ones\n",
    "X = X.drop(['product_category', 'gender', 'address_state', 'time'], axis=1)\n",
    "X = pd.concat([X, product_df, gender_df, state_df, hour_df], axis=1)\n",
    "print(X.info())\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=566571358)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6bb0c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save encoders\n",
    "with open('../model/encoders.pkl', 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'enc_product': enc_product,\n",
    "        'enc_hour': enc_hour,\n",
    "        'enc_gender': enc_gender,\n",
    "        'enc_state': enc_state,\n",
    "        'product_cols': product_cols,\n",
    "        'hour_cols': hour_cols,\n",
    "        'gender_cols': gender_cols,\n",
    "        'state_cols': state_cols,\n",
    "    }, f)\n",
    "\n",
    "print(\"Encoders saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1109b629",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup validation set\n",
    "X, X_valid, y, y_valid = train_test_split(X, y, test_size=0.1)\n",
    "\n",
    "print(X.shape)\n",
    "print(X_valid.shape)\n",
    "print(y.value_counts())\n",
    "print(y_valid.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6b9bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# balance X/y on training set: keep all True, sample smaller number of False\n",
    "pos_idx = y[y].index\n",
    "neg_idx = y[~y].sample(n=len(pos_idx)*5).index\n",
    "\n",
    "balanced_idx = pos_idx.union(neg_idx)\n",
    "X = X.loc[balanced_idx]\n",
    "y = y.loc[balanced_idx]\n",
    "\n",
    "print(y.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e060a1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, roc_auc_score\n",
    "import mlflow\n",
    "import datetime\n",
    "import logging\n",
    "from scipy.stats import norm\n",
    "import pickle\n",
    "import tempfile\n",
    "\n",
    "# Set MLflow tracking server\n",
    "mlflow.set_tracking_uri('http://host.docker.internal:5050')\n",
    "\n",
    "# Enable autologging\n",
    "mlflow.sklearn.autolog()\n",
    "\n",
    "# remove noisy warnings\n",
    "logging.getLogger(\"mlflow.utils.autologging_utils\").setLevel(logging.ERROR)\n",
    "\n",
    "\n",
    "run_name = f\"fraud logistic-regression {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\"\n",
    "with mlflow.start_run(run_name=run_name):\n",
    "  model = LogisticRegression(max_iter=1000, class_weight=\"balanced\")\n",
    "  model.fit(X, y)\n",
    "\n",
    "  # validation performance\n",
    "  y_valid_pred = model.predict(X_valid)\n",
    "  recall = recall_score(y_valid, y_valid_pred)\n",
    "  precision = precision_score(y_valid, y_valid_pred)\n",
    "  f1 = f1_score(y_valid, y_valid_pred)\n",
    "  auc = roc_auc_score(y_valid, y_valid_pred)\n",
    "  t = pd.crosstab(y_valid, y_valid_pred, rownames=['Actual'], colnames=['Predicted'])\n",
    "  hit_rate = min(.999,max(.001,(t[True][True]) / (t[True][False] + t[True][True])))\n",
    "  fa_rate = min(.999,max(.001,(t[False][True]) / (t[False][False] + t[False][True])))\n",
    "  d_prime = norm.ppf(hit_rate) - norm.ppf(fa_rate)\n",
    "  mlflow.log_metric('d_prime', d_prime)\n",
    "\n",
    "  print(t)\n",
    "  print(f\"f1={f1:.3f}, auc={auc:.3f}, recall={recall:.3f}, d_prime={d_prime:.3f}\")\n",
    "\n",
    "  run_id = mlflow.active_run().info.run_id\n",
    "  model_uri = f\"runs:/{run_id}/model\"\n",
    "  mlflow.register_model(model_uri, f\"fraud-logistic-regression\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1caadda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try to get the model from the registry and predict with it\n",
    "name = \"fraud-logistic-regression/4\"\n",
    "model_uri = f\"models:/{name}\"\n",
    "loaded_model = mlflow.sklearn.load_model(model_uri)\n",
    "y_valid_pred_loaded = loaded_model.predict(X_valid)\n",
    "print(\"Predictions from loaded model match original:\", np.array_equal(y_valid_pred, y_valid_pred_loaded))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa13c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, roc_auc_score\n",
    "import mlflow\n",
    "import datetime\n",
    "import logging\n",
    "from scipy.stats import norm\n",
    "import pickle\n",
    "import tempfile\n",
    "\n",
    "# Set MLflow tracking server\n",
    "mlflow.set_tracking_uri('http://host.docker.internal:5050')\n",
    "\n",
    "# Enable autologging\n",
    "mlflow.sklearn.autolog()\n",
    "\n",
    "# remove noisy warnings\n",
    "logging.getLogger(\"mlflow.utils.autologging_utils\").setLevel(logging.ERROR)\n",
    "\n",
    "run_name = f\"fraud random-forest {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\"\n",
    "with mlflow.start_run(run_name=run_name):\n",
    "  for n_esti in range(1, 20):\n",
    "    with mlflow.start_run(nested=True, run_name=f\"{n_esti} estimators\"):\n",
    "      model = RandomForestClassifier(n_estimators=n_esti, class_weight='balanced')\n",
    "      model.fit(X, y)\n",
    "\n",
    "      # validation performance\n",
    "      y_valid_pred = model.predict(X_valid)\n",
    "      recall = recall_score(y_valid, y_valid_pred)\n",
    "      precision = precision_score(y_valid, y_valid_pred)\n",
    "      f1 = f1_score(y_valid, y_valid_pred)\n",
    "      auc = roc_auc_score(y_valid, y_valid_pred)\n",
    "      t = pd.crosstab(y_valid, y_valid_pred, rownames=['Actual'], colnames=['Predicted'])\n",
    "      hit_rate = min(.999,max(.001,(t[True][True]) / (t[True][False] + t[True][True])))\n",
    "      fa_rate = min(.999,max(.001,(t[False][True]) / (t[False][False] + t[False][True])))\n",
    "      d_prime = norm.ppf(hit_rate) - norm.ppf(fa_rate)\n",
    "      mlflow.log_metric('d_prime', d_prime)\n",
    "\n",
    "      print(t)\n",
    "      print(f\"f1={f1:.3f}, auc={auc:.3f}, recall={recall:.3f}, d_prime={d_prime:.3f}\")\n",
    "\n",
    "      run_id = mlflow.active_run().info.run_id\n",
    "      model_uri = f\"runs:/{run_id}/model\"\n",
    "      mlflow.register_model(model_uri, f\"fraud-random-forest-{n_esti}-estimators\")\n",
    "\n",
    "  # Save and log encoders as artifact in the parent run\n",
    "  encoders_data = {\n",
    "      'enc_product': enc_product,\n",
    "      'enc_gender': enc_gender,\n",
    "      'enc_state': enc_state,\n",
    "      'product_cols': product_cols,\n",
    "      'gender_cols': gender_cols,\n",
    "      'state_cols': state_cols,\n",
    "      'model_version': '0.1',\n",
    "      'train_date': datetime.datetime.now().isoformat()\n",
    "  }\n",
    "  with tempfile.NamedTemporaryFile(mode='wb', suffix='.pkl', delete=False) as f:\n",
    "      pickle.dump(encoders_data, f)\n",
    "      encoders_path = f.name\n",
    "  mlflow.log_artifact(encoders_path, artifact_path='encoders')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a30a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, roc_auc_score\n",
    "import mlflow\n",
    "import datetime\n",
    "import logging\n",
    "from scipy.stats import norm\n",
    "import pickle\n",
    "import tempfile\n",
    "\n",
    "# Set MLflow tracking server\n",
    "mlflow.set_tracking_uri('http://host.docker.internal:5050')\n",
    "\n",
    "# Enable autologging\n",
    "mlflow.sklearn.autolog()\n",
    "\n",
    "# remove noisy warnings\n",
    "logging.getLogger(\"mlflow.utils.autologging_utils\").setLevel(logging.ERROR)\n",
    "\n",
    "\n",
    "run_name = f\"fraud support-vector-machine {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\"\n",
    "with mlflow.start_run(run_name=run_name):\n",
    "  C_range = np.logspace(-1, 1, 3, base=10)\n",
    "  gamma_range = np.logspace(-3, -1, 3, base=10)\n",
    "  for C in C_range:\n",
    "    for gamma in gamma_range:\n",
    "      with mlflow.start_run(nested=True, run_name=f\"C={C}, gamma={gamma}\"):\n",
    "        model = SVC(kernel='rbf', C=C, gamma=gamma, class_weight=\"balanced\", max_iter=int(1e8), verbose=False)\n",
    "        model.fit(X, y)\n",
    "\n",
    "        # validation performance\n",
    "        y_valid_pred = model.predict(X_valid)\n",
    "        recall = recall_score(y_valid, y_valid_pred)\n",
    "        precision = precision_score(y_valid, y_valid_pred)\n",
    "        f1 = f1_score(y_valid, y_valid_pred)\n",
    "        auc = roc_auc_score(y_valid, y_valid_pred)\n",
    "        t = pd.crosstab(y_valid, y_valid_pred, rownames=['Actual'], colnames=['Predicted'])\n",
    "        hit_rate = min(.999,max(.001,(t[True][True]) / (t[True][False] + t[True][True])))\n",
    "        fa_rate = min(.999,max(.001,(t[False][True]) / (t[False][False] + t[False][True])))\n",
    "        d_prime = norm.ppf(hit_rate) - norm.ppf(fa_rate)\n",
    "        mlflow.log_metric('d_prime', d_prime)\n",
    "\n",
    "        print(t)\n",
    "        print(f\"f1={f1:.3f}, auc={auc:.3f}, recall={recall:.3f}, d_prime={d_prime:.3f}\")\n",
    "\n",
    "  # Save and log encoders as artifact in the parent run\n",
    "  encoders_data = {\n",
    "      'enc_product': enc_product,\n",
    "      'enc_gender': enc_gender,\n",
    "      'enc_state': enc_state,\n",
    "      'product_cols': product_cols,\n",
    "      'gender_cols': gender_cols,\n",
    "      'state_cols': state_cols,\n",
    "      'model_version': '0.1',\n",
    "      'train_date': datetime.datetime.now().isoformat()\n",
    "  }\n",
    "  with tempfile.NamedTemporaryFile(mode='wb', suffix='.pkl', delete=False) as f:\n",
    "      pickle.dump(encoders_data, f)\n",
    "      encoders_path = f.name\n",
    "  mlflow.log_artifact(encoders_path, artifact_path='encoders')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc373656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Net\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, roc_auc_score\n",
    "import mlflow\n",
    "import datetime\n",
    "import logging\n",
    "from scipy.stats import norm\n",
    "import pickle\n",
    "import tempfile\n",
    "\n",
    "# Set MLflow tracking server\n",
    "mlflow.set_tracking_uri('http://host.docker.internal:5050')\n",
    "\n",
    "# Enable autologging\n",
    "mlflow.sklearn.autolog()\n",
    "\n",
    "# remove noisy warnings\n",
    "logging.getLogger(\"mlflow.utils.autologging_utils\").setLevel(logging.ERROR)\n",
    "\n",
    "\n",
    "run_name = f\"fraud neural-net {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\"\n",
    "with mlflow.start_run(run_name=run_name):\n",
    "  layer1_range = np.logspace(3, 6, 4, base=2)\n",
    "  layer2_range = np.logspace(3, 6, 4, base=2)\n",
    "  for n1 in layer1_range:\n",
    "    for n2 in layer2_range:\n",
    "      with mlflow.start_run(nested=True, run_name=f\"n1={int(n1)}, n2={int(n2)}\"):\n",
    "        model = MLPClassifier(hidden_layer_sizes=[int(n1), int(n2)], learning_rate = \"adaptive\", verbose=False)\n",
    "        model.fit(X, y)\n",
    "\n",
    "        # validation performance\n",
    "        y_valid_pred = model.predict(X_valid)\n",
    "        recall = recall_score(y_valid, y_valid_pred)\n",
    "        precision = precision_score(y_valid, y_valid_pred)\n",
    "        f1 = f1_score(y_valid, y_valid_pred)\n",
    "        auc = roc_auc_score(y_valid, y_valid_pred)\n",
    "        t = pd.crosstab(y_valid, y_valid_pred, rownames=['Actual'], colnames=['Predicted'])\n",
    "        hit_rate = min(.999,max(.001,(t[True][True]) / (t[True][False] + t[True][True])))\n",
    "        fa_rate = min(.999,max(.001,(t[False][True]) / (t[False][False] + t[False][True])))\n",
    "        d_prime = norm.ppf(hit_rate) - norm.ppf(fa_rate)\n",
    "        mlflow.log_metric('d_prime', d_prime)\n",
    "\n",
    "        print(t)\n",
    "        print(f\"f1={f1:.3f}, auc={auc:.3f}, recall={recall:.3f}, d_prime={d_prime:.3f}\")\n",
    "\n",
    "  # Save and log encoders as artifact in the parent run\n",
    "  encoders_data = {\n",
    "      'enc_product': enc_product,\n",
    "      'enc_gender': enc_gender,\n",
    "      'enc_state': enc_state,\n",
    "      'product_cols': product_cols,\n",
    "      'gender_cols': gender_cols,\n",
    "      'state_cols': state_cols,\n",
    "      'model_version': '0.1',\n",
    "      'train_date': datetime.datetime.now().isoformat()\n",
    "  }\n",
    "  with tempfile.NamedTemporaryFile(mode='wb', suffix='.pkl', delete=False) as f:\n",
    "      pickle.dump(encoders_data, f)\n",
    "      encoders_path = f.name\n",
    "  mlflow.log_artifact(encoders_path, artifact_path='encoders')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
